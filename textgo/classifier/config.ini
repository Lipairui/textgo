[FastText]
num_classes = 2 
 
num_epochs = 100

learning_rate = 0.1

# 损失函数 {ns, hs, softmax} 
loss_function = "hs"

wordNgrams = 3

random_seed = 1

# 词向量维度 
embed = 100

# 语言：中文(zh)/英文(en) 
lang = 'zh'

# 是否需要对文本进行预处理 
preprocess = True

# 切词方式：词(True)/字(False) 
word_level = True

[TextCNN]
dropout = 0.5 

# 若超过1000batch效果还没提升，则提前结束训练
require_improvement = 300 

num_classes = 2

num_epochs = 15

batch_size = 128

# 每句话处理成的长度(短填长切) 
max_len = 64

learning_rate = 1e-3 

# 卷积核尺寸
filter_sizes = (2, 3, 4)

# 卷积核数量(channels数)
num_filters = 256

# 词向量维度
embed = 100

random_seed = 1

# 切词方式：词(True)/字(False)
word_level = False

# 是否需要对文本进行预处理
preprocess = True

# 语言：中文(zh)/英文(en)
lang = 'zh'

vocab_path = "./vocab/vocab_zh.bin"

max_vocab_size = 10000

# pretrained embedding file path, "random" means not using pretrained 
#embedding = "./w2v_models/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding"
embedding = "random"

# 训练时每隔XX steps在验证集上进行评估
evaluation_steps = 1
# 30


[TextRCNN]
dropout = 0.5 

# 若超过1000batch效果还没提升，则提前结束训练
require_improvement = 300 

num_classes = 2

num_epochs = 15

batch_size = 128

# 每句话处理成的长度(短填长切) 
max_len = 64

learning_rate = 1e-3 

# LSTM隐藏层
hidden_size = 256

# LSTM层数
num_layers = 1

# 字向量维度
embed = 100

random_seed = 1

# 切词方式：词(True)/字(False)
word_level = False

# 是否需要对文本进行预处理
preprocess = True

# 语言：中文(zh)/英文(en)
lang = 'zh'

vocab_path = "./vocab/vocab_zh.bin"

max_vocab_size = 10000

# pretrained embedding file path, "random" means not using pretrained 
#embedding = "./w2v_models/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding"
embedding = "random"

# 训练时每隔XX steps在验证集上进行评估
evaluation_steps = 30


[TextRNN]
dropout = 0.5 

# 若超过1000batch效果还没提升，则提前结束训练
require_improvement = 300 

num_classes = 2

num_epochs = 15

batch_size = 128

# 每句话处理成的长度(短填长切) 
max_len = 64

learning_rate = 1e-3 

# LSTM隐藏层
hidden_size = 256

# LSTM层数
num_layers = 1

# 字向量维度
embed = 100

random_seed = 1

# 切词方式：词(True)/字(False)
word_level = False

# 是否需要对文本进行预处理
preprocess = True

# 语言：中文(zh)/英文(en)
lang = 'zh'

vocab_path = "./vocab/vocab_zh.bin"

max_vocab_size = 10000

# pretrained embedding file path, "random" means not using pretrained 
#embedding = "./w2v_models/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding"
embedding = "random"

# 训练时每隔XX steps在验证集上进行评估
evaluation_steps = 30


[TextRNN_Att]
dropout = 0.5 

# 若超过1000batch效果还没提升，则提前结束训练
require_improvement = 300 

num_classes = 2

num_epochs = 15

batch_size = 128

# 每句话处理成的长度(短填长切) 
max_len = 64

learning_rate = 1e-3 

# LSTM隐藏层
hidden_size = 128

# 全连接层2
hidden_size2 = 64

# LSTM层数
num_layers = 2

# 字向量维度
embed = 100

random_seed = 1

# 切词方式：词(True)/字(False)
word_level = False

# 是否需要对文本进行预处理
preprocess = True

# 语言：中文(zh)/英文(en)
lang = 'zh'

#vocab_path = "./vocab/vocab_rnn_att.txt"
vocab_path = "./vocab/vocab_zh.bin"

max_vocab_size = 10000

# pretrained embedding file path, "random" means not using pretrained 
#embedding = "./w2v_models/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding"
embedding = "random"

# 训练时每隔XX steps在验证集上进行评估
evaluation_steps = 30

[Bert]
# 若超过1000batch效果还没提升，则提前结束训练 
require_improvement = 180

num_classes = 2 
 
num_epochs = 40 
# 40

batch_size = 64
 
# 每句话处理成的长度(短填长切)  
max_len = 64 
 
learning_rate = 2e-5

# 训练时loss是否使用类别分布的先验信息prior，主要用于解决样本不均衡问题
loss_with_prior = False

# 训练时每隔XX steps在验证集上进行评估 
evaluation_steps = 30 
# 30

# 评估时使用的指标, dev_loss/dev_class0_precision/...
val_metric = "dev_loss"

# 预训练模型绝对地址
pretrained_model = "./pretrained_models/bert-base-chinese-pytorch"

random_seed = 1 


[XLNet]
# 若超过1000batch效果还没提升，则提前结束训练 
require_improvement = 180

num_classes = 2 
 
num_epochs = 40
 
batch_size = 64
 
# 每句话处理成的长度(短填长切)  
max_len = 64 
 
learning_rate = 2e-5

# 训练时loss是否使用类别分布的先验信息prior，主要用于解决样本不均衡问题
loss_with_prior = False

# 训练时每隔XX steps在验证集上进行评估 
evaluation_steps = 30

# 评估时使用的指标, dev_loss/dev_class0_precision/...
val_metric = "dev_loss"

pretrained_model = "./pretrained_models/xlnet-base-chinese-pytorch"

random_seed = 1 
